\documentclass[12pt,a4paper]{report}
\usepackage{cclicenses}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\title{Ceph Training Materials}
\author{Victor Denisov}
\date{\today}
\begin{document}
   \maketitle
\begin{abstract}
This work, ``Ceph Training Materials'', is derivative of ``Ceph Documentation'' http://docs.ceph.com/docs/master/ by Victor Denisov, used under \bysa. ``Ceph Documentation'' is licensed under \bysa by Inktank.
\end{abstract}

% Makes emph bold by default.
\makeatletter
\DeclareRobustCommand{\em}{%
  \@nomath\em \if b\expandafter\@car\f@series\@nil
  \normalfont \else \bfseries \fi}
\makeatother

\chapter{Ceph Architecture}
Ceph uniquely delivers object, block, and file storage in one unified system.
A Ceph Node leverages commodity hardware and intelligent daemons, and a Ceph
Storage Cluster accommodates large numbers of nodes, which communicate with
each other to replicate and redistribute data dynamically. All the components
of ceph storage cluster are presented on figure \ref{fig:ceph_architecture}

\begin{figure}[h]
	\includegraphics[scale=0.5]{stack.png}
	\caption{Ceph Architecture}
	\label{fig:ceph_architecture}
\end{figure}

\section{The Ceph Storage Cluster}
Ceph provides an infinitely scalable Ceph Storage Cluster based upon RADOS.
RADOS means Reliable Autonomic Distributed Object Store.

A Ceph Storage Cluster consists of two types of daemons:

\begin{itemize}
	\item Ceph Monitor
	\item Ceph OSD Daemon
\end{itemize}

A Ceph Monitor maintains a master copy of the cluster map. A cluster of Ceph
monitors ensures high availability should a monitor daemon fail. Storage
cluster clients retrieve a copy of the cluster map from the Ceph Monitor.

A Ceph OSD Daemon checks its own state and the state of other OSDs and reports
back to monitors.

Storage cluster clients and each Ceph OSD Daemon use the CRUSH algorithm to
efficiently compute information about data location, instead of having to
depend on a central lookup table. Ceph’s high-level features include providing
a native interface to the Ceph Storage Cluster via librados, and a number of
service interfaces built on top of librados.

\subsection{Storing Data}

The Ceph Storage Cluster receives data from Ceph Clients --- whether it comes
through a Ceph Block Device, Ceph Object Storage, the Ceph Filesystem or a
custom implementation you create using librados --- and it stores the data as
objects. Each object corresponds to a file in a filesystem, which is stored on
an Object Storage Device. Ceph OSD Daemons handle the read/write operations on
the storage disks.

\begin{figure}[h]
	\includegraphics[scale=0.75]{object_file_disk.png}
	\label{fig:object_file_disk}
\end{figure}

Ceph OSD Daemons store all data as objects in a flat namespace (e.g., no
hierarchy of directories). An object has an identifier, binary data, and
metadata consisting of a set of name/value pairs. The semantics are completely
up to Ceph Clients. For example, CephFS uses metadata(figure
\ref{fig:object_metadata}) to store file attributes such as the file owner,
created date, last modified date, and so forth. \emph{Every object has an id which is
unique across the entire ceph cluster, not just the local filesystem.}

\begin{figure}[h]
	\includegraphics[scale=0.60]{object_metadata.png}
	\caption{Object's Metadata}
	\label{fig:object_metadata}
\end{figure}

\subsection{Scalability and High Availability} \label{scal_and_high_avail}

In traditional architectures, clients talk to a centralized component (e.g., a
gateway, broker, API, facade, etc.), which acts as a single point of entry to a
complex subsystem. This imposes a limit to both performance and scalability,
while introducing a single point of failure (i.e., if the centralized component
goes down, the whole system goes down, too).

Ceph eliminates the centralized gateway to enable clients to interact with Ceph
OSD Daemons directly. Ceph OSD Daemons create object replicas on other Ceph
Nodes to ensure data safety and high availability. Ceph also uses a cluster of
monitors to ensure high availability. To eliminate centralization, Ceph uses an
algorithm called CRUSH.

\subsection{CRUSH Introduction}

Ceph Clients and Ceph OSD Daemons both use the CRUSH algorithm to efficiently
compute information about object location, instead of having to depend on a
central lookup table. CRUSH provides a better data management mechanism
compared to older approaches, and enables massive scale by cleanly distributing
the work to all the clients and OSD daemons in the cluster. CRUSH uses
intelligent data replication to ensure resiliency, which is better suited to
hyper-scale storage. The following sections provide additional details on how
CRUSH works. For a detailed discussion of CRUSH, see CRUSH - Controlled,
Scalable, Decentralized Placement of Replicated Data. \ref{crush_chapter}

\subsection{Cluster Map} \label{cluster_map}

Ceph depends upon Ceph Clients and Ceph OSD Daemons having knowledge of the
cluster topology, which is inclusive of 5 maps collectively referred to as the
``Cluster Map'':
\begin{enumerate}
	\item \emph{The Monitor Map}: Contains the cluster fsid, the position,
		name address and port of each monitor. It also indicates the
		current epoch, when the map was created, and the last time it
		changed. To view a monitor map, execute \emph{ceph mon dump}.
	\item \emph{The OSD Map}: Contains the cluster fsid, when the map was
		created and last modified, a list of pools, replica sizes, PG
		numbers, a list of OSDs and their status (e.g., up, in). To
		view an OSD map, execute \emph{ceph osd dump}.
	\item \emph{The PG Map}: Contains the PG version, its time stamp, the
		last OSD map epoch, the full ratios, and details on each
		placement group such as the PG ID, the Up Set, the Acting Set,
		the state of the PG (e.g., active + clean), and data usage
		statistics for each pool. To view a PG map, execute \emph{ceph pg dump}
	\item \emph{The CRUSH Map}: Contains a list of storage devices, the
		failure domain hierarchy (e.g., device, host, rack, row, room,
		etc.), and rules for traversing the hierarchy when storing
		data. To view a CRUSH map, execute \emph{ceph osd getcrushmap -o
		filename;} then, decompile it by executing \emph{crushtool -d
		comp-crushmap-filename -o decomp-crushmap-filename}. You can
		view the decompiled map in a text editor or with cat.
\end{enumerate}

Each map maintains an iterative history of its operating state changes. Ceph
Monitors maintain a master copy of the cluster map including the cluster
members, state, changes, and the overall health of the Ceph Storage Cluster.

\subsection{High Availability Monitors}

Before Ceph Clients can read or write data, they must contact a Ceph Monitor to
obtain the most recent copy of the cluster map. A Ceph Storage Cluster can
operate with a single monitor; however, this introduces a single point of
failure (i.e., if the monitor goes down, Ceph Clients cannot read or write
data).

For added reliability and fault tolerance, Ceph supports a cluster of monitors.
In a cluster of monitors, latency and other faults can cause one or more
monitors to fall behind the current state of the cluster. For this reason, Ceph
must have agreement among various monitor instances regarding the state of the
cluster. Ceph always uses a majority of monitors (e.g., 1, 2:3, 3:5, 4:6, etc.)
and the Paxos algorithm to establish a consensus among the monitors about the
current state of the cluster.

\subsection{High Availability Authentication}
To identify users and protect against man-in-the-middle attacks, Ceph provides
its cephx authentication system to authenticate users and daemons.  \emph{The
cephx protocol does not address data encryption in transport (e.g., SSL/TLS) or
encryption at rest.}

Cephx uses shared secret keys for authentication, meaning both the client and
the monitor cluster have a copy of the client’s secret key. The authentication
protocol is such that both parties are able to prove to each other they have a
copy of the key without actually revealing it. This provides mutual
authentication, which means the cluster is sure the user possesses the secret
key, and the user is sure that the cluster has a copy of the secret key.

A key scalability feature of Ceph is to avoid a centralized interface to the
Ceph object store, which means that Ceph clients must be able to interact with
OSDs directly. To protect data, Ceph provides its cephx authentication system,
which authenticates users operating Ceph clients. The cephx protocol operates
in a manner with behavior similar to Kerberos.

A user/actor invokes a Ceph client to contact a monitor. Unlike Kerberos, each
monitor can authenticate users and distribute keys, so there is no single point
of failure or bottleneck when using cephx. The monitor returns an
authentication data structure similar to a Kerberos ticket that contains a
session key for use in obtaining Ceph services. This session key is itself
encrypted with the user’s permanent secret key, so that only the user can
request services from the Ceph monitor(s). The client then uses the session key
to request its desired services from the monitor, and the monitor provides the
client with a ticket that will authenticate the client to the OSDs that
actually handle data. Ceph monitors and OSDs share a secret, so the client can
use the ticket provided by the monitor with any OSD or metadata server in the
cluster. Like Kerberos, cephx tickets expire, so an attacker cannot use an
expired ticket or session key obtained surreptitiously. This form of
authentication will prevent attackers with access to the communications medium
from either creating bogus messages under another user’s identity or altering
another user’s legitimate messages, as long as the user’s secret key is not
divulged before it expires.

To use cephx, an administrator must set up users first. In the following
diagram, the client.admin user invokes ceph auth get-or-create-key from the
command line to generate a username and secret key. Ceph’s auth subsystem
generates the username and key, stores a copy with the monitor(s) and transmits
the user’s secret back to the client.admin user. This means that the client and
the monitor share a secret key.

\emph{The client.admin user must provide the user ID and secret key to the user in a
secure manner.}

\begin{figure}[h]
	\includegraphics[scale=0.75]{create_user.png}
	\caption{User Creation}
	\label{fig:create_user}
\end{figure}

To authenticate with the monitor, the client passes in the user name to the
monitor, and the monitor generates a session key and encrypts it with the
secret key associated to the user name. Then, the monitor transmits the
encrypted ticket back to the client. The client then decrypts the payload with
the shared secret key to retrieve the session key. The session key identifies
the user for the current session. The client then requests a ticket on behalf
of the user signed by the session key. The monitor generates a ticket, encrypts
it with the user’s secret key and transmits it back to the client. The client
decrypts the ticket and uses it to sign requests to OSDs and metadata servers
throughout the cluster.

\begin{figure}[h]
	\includegraphics[scale=0.75]{ticket_retrieval.png}
	\caption{Ticket Retrieval}
	\label{fig:ticket_retrieval}
\end{figure}

The cephx protocol authenticates ongoing communications between the client
machine and the Ceph servers. Each message sent between a client and server,
subsequent to the initial authentication, is signed using a ticket that the
monitors, OSDs and metadata servers can verify with their shared secret.

\begin{figure}[h]
	\includegraphics[scale=0.60]{request_to_osd.png}
	\caption{Secret Verification}
	\label{fig:ticket_retrieval}
\end{figure}

The protection offered by this authentication is between the Ceph client and
the Ceph server hosts. The authentication is not extended beyond the Ceph
client. If the user accesses the Ceph client from a remote host, Ceph
authentication is not applied to the connection between the user’s host and the
client host.

\subsection{Smart Daemons Enable Hyperscale}

In many clustered architectures, the primary purpose of cluster membership is
so that a centralized interface knows which nodes it can access. Then the
centralized interface provides services to the client through a double
dispatch–which is a huge bottleneck at the petabyte-to-exabyte scale.

Ceph eliminates the bottleneck: Ceph’s OSD Daemons AND Ceph Clients are cluster
aware. Like Ceph clients, each Ceph OSD Daemon knows about other Ceph OSD
Daemons in the cluster. This enables Ceph OSD Daemons to interact directly with
other Ceph OSD Daemons and Ceph monitors. Additionally, it enables Ceph Clients
to interact directly with Ceph OSD Daemons.

The ability of Ceph Clients, Ceph Monitors and Ceph OSD Daemons to interact
with each other means that Ceph OSD Daemons can utilize the CPU and RAM of the
Ceph nodes to easily perform tasks that would bog down a centralized server.
The ability to leverage this computing power leads to several major benefits:

\begin{enumerate}
	\item \emph{OSDs Service Clients Directly}: Since any network device
		has a limit to the number of concurrent connections it can
		support, a centralized system has a low physical limit at high
		scales. By enabling Ceph Clients to contact Ceph OSD Daemons
		directly, Ceph increases both performance and total system
		capacity simultaneously, while removing a single point of
		failure. Ceph Clients can maintain a session when they need to,
		and with a particular Ceph OSD Daemon instead of a centralized
		server.

	\item \emph{OSD Membership and Status}: Ceph OSD Daemons join a cluster and
		report on their status. At the lowest level, the Ceph OSD Daemon status
		is up or down reflecting whether or not it is running and able to
		service Ceph Client requests. If a Ceph OSD Daemon is down and in the
		Ceph Storage Cluster, this status may indicate the failure of the Ceph
		OSD Daemon. If a Ceph OSD Daemon is not running (e.g., it crashes), the
		Ceph OSD Daemon cannot notify the Ceph Monitor that it is down. The
		Ceph Monitor can ping a Ceph OSD Daemon periodically to ensure that it
		is running. However, Ceph also empowers Ceph OSD Daemons to determine
		if a neighboring OSD is down, to update the cluster map and to report
		it to the Ceph monitor(s). This means that Ceph monitors can remain
		light weight processes. See Monitoring OSDs and Heartbeats for
		additional details.

	\item \emph{Data Scrubbing}: As part of maintaining data consistency
		and cleanliness, Ceph OSD Daemons can scrub objects within
		placement groups. That is, Ceph OSD Daemons can compare object
		metadata in one placement group with its replicas in placement
		groups stored on other OSDs. Scrubbing (usually performed
		daily) catches bugs or filesystem errors. Ceph OSD Daemons also
		perform deeper scrubbing by comparing data in objects
		bit-for-bit. Deep scrubbing (usually performed weekly) finds
		bad sectors on a drive that weren't apparent in a light scrub.
		See Data Scrubbing for details on configuring scrubbing.

	\item \emph{Replication}: Like Ceph Clients, Ceph OSD Daemons use the
		CRUSH algorithm, but the Ceph OSD Daemon uses it to compute
		where replicas of objects should be stored (and for
		rebalancing). In a typical write scenario, a client uses the
		CRUSH algorithm to compute where to store an object, maps the
		object to a pool and placement group, then looks at the CRUSH
		map to identify the primary OSD for the placement group.  The
		client writes the object to the identified placement group in
		the primary OSD. Then, the primary OSD with its own copy of the
		CRUSH map identifies the secondary and tertiary OSDs for
		replication purposes, and replicates the object to the
		appropriate placement groups in the secondary and tertiary OSDs
		(as many OSDs as additional replicas), and responds to the
		client once it has confirmed the object was stored
		successfully.
\end{enumerate}

\begin{figure}[h]
	\includegraphics[scale=0.60]{replication.png}
	\caption{OSD Replication}
	\label{fig:osd_replication}
\end{figure}

With the ability to perform data replication, Ceph OSD Daemons relieve Ceph
clients from that duty, while ensuring high data availability and data safety.

\subsection{Dynamic Cluster Management}

In the section \ref{scal_and_high_avail}, we explained how Ceph uses CRUSH,
cluster awareness and intelligent daemons to scale and maintain high
availability. Key to Ceph’s design is the autonomous, self-healing, and
intelligent Ceph OSD Daemon. Let’s take a deeper look at how CRUSH works to
enable modern cloud storage infrastructures to place data, rebalance the
cluster and recover from faults dynamically.

\subsection{About Pools}
The Ceph storage system supports the notion of 'Pools', which are logical
partitions for storing objects.

Ceph Clients retrieve a Cluster Map(\ref{cluster_map}) from a Ceph Monitor, and
write objects to pools. The pool's size or number of replicas, the CRUSH
ruleset and the number of placement groups determine how Ceph will place the
data.

\begin{figure}[h]
	\includegraphics[scale=0.75]{pool_usage.png}
	\caption{Writing Data to Pools}
	\label{fig:pool_usage}
\end{figure}

Pools set at least the following parameters:

\begin{itemize}
	\item Ownership/Access to Objects
	\item The Number of Placement Groups, and
	\item The CRUSH Ruleset to Use
\end{itemize}

\subsection{Mapping PGs to OSDs}

Each pool has a number of placement groups. CRUSH maps PGs to OSDs dynamically.
When a Ceph Client stores objects, CRUSH will map each object to a placement
group.

Mapping objects to placement groups creates a layer of indirection between the
Ceph OSD Daemon and the Ceph Client. The Ceph Storage Cluster must be able to
grow (or shrink) and rebalance where it stores objects dynamically. If the Ceph
Client “knew” which Ceph OSD Daemon had which object, that would create a tight
coupling between the Ceph Client and the Ceph OSD Daemon. Instead, the CRUSH
algorithm maps each object to a placement group and then maps each placement
group to one or more Ceph OSD Daemons. This layer of indirection allows Ceph to
rebalance dynamically when new Ceph OSD Daemons and the underlying OSD devices
come online. Diagram(\ref{fig:pg_mapping}) depicts how CRUSH maps objects to
placement groups, and placement groups to OSDs.

\begin{figure}[h]
	\includegraphics[scale=0.60]{pg_mapping.png}
	\caption{PG mapping}
	\label{fig:pg_mapping}
\end{figure}

With a copy of the cluster map and the CRUSH algorithm, the client can compute
exactly which OSD to use when reading or writing a particular object.

\subsection{Calculating PG IDs}\label{calculating_pg_ids}

When a Ceph Client binds to a Ceph Monitor, it retrieves the latest copy of the
Cluster Map. With the cluster map, the client knows about all of the monitors,
OSDs, and metadata servers in the cluster. However, it doesn't know anything
about object locations.

Object locations get computed.

The only input required by the client is the object ID and the pool. It’s
simple: Ceph stores data in named pools (e.g., ``liverpool''). When a client
wants to store a named object (e.g., ``john,'' ``paul,'' ``george,'' ``ringo'', etc.)
it calculates a placement group using the object name, a hash code, the number
of PGs in the pool and the pool name. Ceph clients use the following steps to
compute PG IDs.

\begin{enumerate}
	\item The client inputs the pool ID and the object ID. (e.g., pool =
		“liverpool” and object-id = “john”)
	\item Ceph takes the object ID and hashes it.
	\item Ceph calculates the hash modulo the number of PGs. (e.g., 58) to
		get a PG ID.
	\item Ceph gets the pool ID given the pool name (e.g., “liverpool” = 4)
	\item Ceph prepends the pool ID to the PG ID (e.g., 4.58).
\end{enumerate}

Computing object locations is much faster than performing object location query
over a chatty session. The CRUSH algorithm allows a client to compute where
objects should be stored, and enables the client to contact the primary OSD to
store or retrieve the objects.

\subsection{Peering and Sets}

In previous sections, we noted that Ceph OSD Daemons check each others
heartbeats and report back to the Ceph Monitor. Another thing Ceph OSD daemons
do is called ‘peering’, which is the process of bringing all of the OSDs that
store a Placement Group (PG) into agreement about the state of all of the
objects (and their metadata) in that PG. In fact, Ceph OSD Daemons Report
Peering Failure to the Ceph Monitors. Peering issues usually resolve
themselves; however, if the problem persists, you may need to refer to the
Troubleshooting Peering Failure section. \emph{Agreeing on the state does not mean
that the PGs have the latest contents.}

The Ceph Storage Cluster was designed to store at least two copies of an object
(i.e., size = 2), which is the minimum requirement for data safety. For high
availability, a Ceph Storage Cluster should store more than two copies of an
object (e.g., size = 3 and min size = 2) so that it can continue to run in a
degraded state while maintaining data safety.

Referring back to the diagram \ref{fig:osd_replication}, we do not
name the Ceph OSD Daemons specifically (e.g., osd.0, osd.1, etc.), but rather
refer to them as Primary, Secondary, and so forth. By convention, the Primary
is the first OSD in the Acting Set, and is responsible for coordinating the
peering process for each placement group where it acts as the Primary, and is
the ONLY OSD that that will accept client-initiated writes to objects for a
given placement group where it acts as the Primary.

When a series of OSDs are responsible for a placement group, that series of
OSDs, we refer to them as an Acting Set. An Acting Set may refer to the Ceph
OSD Daemons that are currently responsible for the placement group, or the Ceph
OSD Daemons that were responsible for a particular placement group as of some
epoch.

The Ceph OSD daemons that are part of an Acting Set may not always be up. When
an OSD in the Acting Set is up, it is part of the Up Set. The Up Set is an
important distinction, because Ceph can remap PGs to other Ceph OSD Daemons
when an OSD fails.

\emph{In an Acting Set for a PG containing osd.25, osd.32 and osd.61, the first
OSD, osd.25, is the Primary. If that OSD fails, the Secondary, osd.32,
becomes the Primary, and osd.25 will be removed from the Up Set.}

\subsection{Rebalancing}
When you add a Ceph OSD Daemon to a Ceph Storage Cluster, the cluster map gets
updated with the new OSD. Referring back to \ref{calculating_pg_ids}, this changes
the cluster map. Consequently, it changes object placement, because it changes
an input for the calculations. Diagram \ref{fig:pg_rebalancing} depicts the rebalancing
process (albeit rather crudely, since it is substantially less impactful with
large clusters) where some, but not all of the PGs migrate from existing OSDs
(OSD 1, and OSD 2) to the new OSD (OSD 3). Even when rebalancing, CRUSH is
stable. Many of the placement groups remain in their original configuration,
and each OSD gets some added capacity, so there are no load spikes on the new
OSD after rebalancing is complete.

\begin{figure}[h]
	\includegraphics[scale=0.60]{pg_rebalancing.png}
	\caption{PG rebalancing}
	\label{fig:pg_rebalancing}
\end{figure}

\subsection{Data Consistency}

As part of maintaining data consistency and cleanliness, Ceph OSDs can also
scrub objects within placement groups. That is, Ceph OSDs can compare object
metadata in one placement group with its replicas in placement groups stored in
other OSDs. Scrubbing (usually performed daily) catches OSD bugs or filesystem
errors. OSDs can also perform deeper scrubbing by comparing data in objects
bit-for-bit. Deep scrubbing (usually performed weekly) finds bad sectors on a
disk that weren't apparent in a light scrub.

\section{Ceph Protocol}

Ceph Clients use the native protocol for interacting with the Ceph Storage
Cluster. Ceph packages this functionality into the librados library so that you
can create your own custom Ceph Clients. Diagram \ref{fig:librados} depicts the
basic architecture.

\begin{figure}[h]
	\includegraphics[scale=0.60]{librados.png}
	\caption{Librados}
	\label{fig:librados}
\end{figure}

\subsection{Native Protocol and librados}

Modern applications need a simple object storage interface with asynchronous
communication capability. The Ceph Storage Cluster provides a simple object
storage interface with asynchronous communication capability. The interface
provides direct, parallel access to objects throughout the cluster.

\begin{itemize}
	\item Pool Operations
	\item Snapshots and Copy-on-write Cloning
	\item Read/Write Objects - Create or Remove - Entire Object or Byte Range - Append or Truncate
	\item Create/Set/Get/Remove XATTRs
	\item Create/Set/Get/Remove Key/Value Pairs
	\item Compound operations and dual-ack semantics
	\item Object Classes
\end{itemize}

\section{Ceph Clients}
Ceph Clients include a number of service interfaces. These include:

\begin{itemize}
	\item \emph{Block Devices}: The Ceph Block Device (a.k.a., RBD) service
		provides resizable, thin-provisioned block devices with
		snapshotting and cloning. Ceph stripes a block device across
		the cluster for high performance. Ceph supports both kernel
		objects (KO) and a QEMU hypervisor that uses librbd
		directly–avoiding the kernel object overhead for virtualized
		systems.

	\item \emph{Object Storage}: The Ceph Object Storage (a.k.a., RGW)
		service provides RESTful APIs with interfaces that are
		compatible with Amazon S3 and OpenStack Swift.

	\item \emph{Filesystem}: The Ceph Filesystem (CephFS) service provides
		a POSIX compliant filesystem usable with mount or as a
		filesytem in user space (FUSE).
\end{itemize}

Ceph can run additional instances of OSDs, MDSs, and monitors for scalability
and high availability. Diagram \ref{fig:client_architecture} depicts the
high-level architecture.

\begin{figure}[h]
	\includegraphics[scale=0.60]{client_architecture.png}
	\caption{Ceph Client Architecture}
	\label{fig:client_architecture}
\end{figure}

\subsection{Ceph Object Storage}

The Ceph Object Storage daemon, radosgw, is a FastCGI service that provides a
RESTful HTTP API to store objects and metadata. It layers on top of the Ceph
Storage Cluster with its own data formats, and maintains its own user database,
authentication, and access control. The RADOS Gateway uses a unified namespace,
which means you can use either the OpenStack Swift-compatible API or the Amazon
S3-compatible API. For example, you can write data using the S3-compatible API
with one application and then read data using the Swift-compatible API with
another application.

\subsubsection{S3/Swift Objects and Store Cluster Objects Compared}

Ceph’s Object Storage uses the term object to describe the data it stores. S3
and Swift objects are not the same as the objects that Ceph writes to the Ceph
Storage Cluster. Ceph Object Storage objects are mapped to Ceph Storage Cluster
objects. The S3 and Swift objects do not necessarily correspond in a 1:1 manner
with an object stored in the storage cluster. It is possible for an S3 or Swift
object to map to multiple Ceph objects.

\section{The CRUSH Algorithm}

The CRUSH algorithm distributes data objects among storage devices according to
a per-device weight value, approximating a uniform probability distribution.
The distribution is controlled by a hierarchical cluster map representing the
available storage resources and composed of the logical elements from which it
is built.  For example, one might de- scribe a large installation in terms of
rows of server cabinets, cabinets filled with disk shelves, and shelves filled
with storage devices. The data distribution policy is defined in terms of
placement rules that specify how many replica targets are chosen from the
cluster  and what restrictions are  imposed on replica placement.  For example,
one might specify that three mirrored replicas are  to be  placed on devices in
different physical cabinets so that they do not share the same electrical
circuit.

Given a single integer input value $x$, CRUSH will output an ordered list
$\vec{R}$ of $n$ distinct storage targets. CRUSH utilizes a strong multi-input
integer hash function whose inputs include $x$, making the mapping completely
deterministic and independently calculable using only the cluster map,
placement rules, and $x$. The distribution is pseudo-random in that there is no
apparent correlation between the resulting output from similar inputs or in the
items stored on any storage device. We say that CRUSH generates a declustered
distribution of replicas in that the set of devices sharing replicas for one
item also appears to be independent of all other items

\subsection{Hierarchical Cluster Map}

The cluster map is composed of devices and buckets, both of which have
numerical identifiers and weight values associated with them. Buckets can
contain any number of devices or other buckets, allowing them to form interior
nodes in a storage hierarchy in which devices are always at the leaves. Storage
devices are assigned weights by the administrator to control the relative
amount of data they are responsible for storing.  Although a large system will
likely contain devices with a variety of capacity and performance
characteristics, randomized  data  distributions  statistically  correlate
device utilization with workload, such that device load is on average
proportional to the amount of data stored. This results in a one-dimensional
placement metric, weight, which should be derived from the device's
capabilities. Bucket weights are defined as the sum of the weights of the items
they contain. Buckets can be composed arbitrarily to construct a hierarchy
representing available storage. For example, one might create a cluster map
with ``shelf'' buckets at the lowest level to  represent sets  of  identical
devices as  they  are  installed, and  then  combine shelves into ``cabinet''
buckets to  group together shelves that are installed in the same rack.
Cabinets might be further grouped into ``row'' or ``room'' buckets for a large
system.  Data is placed in the hierarchy by recursively selecting nested bucket
items via a pseudo-random hash-like function. In contrast to conventional
hashing techniques, in which any change in the number of target bins (devices)
results  in  a  massive  reshuffling  of  bin  contents,  CRUSH  is based on
four different bucket types,  each with a different selection algorithm to
address data movement resulting from the addition or removal of devices and
overall computational complexity.

\subsection{Replica Placement}

CRUSH  is  designed  to  distribute  data  uniformly  among weighted devices to
maintain a statistically balanced utilization of storage and device bandwidth
resources. The placement of replicas on storage devices in the hierarchy can
also have a  critical effect on data safety. By reflecting the underlying
physical organization of the installation, CRUSH can model --- and thereby
address --- potential sources of correlated  device  failures. Typical sources
include physical proximity, a shared power source, and a shared network. By
encoding this information into the cluster map, CRUSH placement policies can
separate object replicas across different failure domains while still
maintaining the desired distribution. For example, to address the possibility
of concurrent failures, it may be desirable to ensure that data replicas are on
devices in different shelves, racks, power supplies, controllers, and/or
physical locations.

In order to accommodate the wide variety of scenarios in which CRUSH might be
used, both in terms of data replication strategies and underlying hardware
configurations, CRUSH defines placement rules for each replication strategy or
distribution policy employed that allow the storage system or administrator to
specify exactly how object replicas are placed. For example, one might have a
rule selecting a pair of targets for 2-way mirroring, one for selecting three
targets in two different data centers for 3-way mirroring, one for RAID-4 over
six storage devices, and so on.

\begin{algorithm}
\caption{CRUSH placement for object $x$}
\label{algorithm:1}
\begin{algorithmic}[1]
\Procedure {take}{$a$}
	\State $i \gets [a]$
\EndProcedure

\Procedure {SELECT}{n,t} \Comment{Select $n$ items of type $t$}
	\State $\vec{a} \gets \emptyset$
	\For {$i \in \vec{i}$}
		\State $f \gets 0$ \Comment{No failures yet}
		\For {$r \gets 1,n$}
			\State $f_r \gets 0$ \Comment{No failures on this replica}
			\State $retry\_descent \gets false$
			\Repeat
				\State $ b \gets bucket(i) $
				\State $ retry\_bucket \gets false $
				\Repeat
					\If {``first n''}
						\State $r' \gets r + f$
					\Else
						\State $r' \gets r + f_rn$
					\EndIf
					\State $o \gets b.c(r',x)$ \Comment{c depends on bucket type. See \ref{bucket_types}}
					\If {$type(o) \neq t$}
						\State $b \gets bucket(o)$
						\State $retry\_bucket \gets true$
					\ElsIf {$o \in \vec{o}$ or $failed(o)$ or $overload(o,x)$}
						\State $f_r \gets f_r + 1, f \gets f + 1$
						\If{$o \in \vec{o}$ and $f_r < 3$}
							\State $retry\_bucket \gets true$
						\Else
							\State $retry\_descent \gets true$
						\EndIf
					\EndIf
				\Until {$\neg retry\_bucket$}
			\Until {$\neg retry\_descent$}
			\State $\vec{o} \gets [ \vec{o}, o ]$
		\EndFor
	\EndFor
	\State $\vec{i} \gets \vec{o}$
\EndProcedure

\Procedure {EMIT}{}
	\State $\vec{R} \gets [ \vec{R},\vec{i} ] $
\EndProcedure
\end{algorithmic}
\end{algorithm}

Each rule consists of a sequence of operations applied to the hierarchy in a
simple execution environment, presented as pseudocode in Algorithm \ref{algorithm:1}. The
integer input to the CRUSH function, $x$, is typically an object name or other
identifier, such as an identifier for a group of objects whose replicas will be
placed on the same devices. The $take(a)$ operation selects an item (typically
a bucket) within the storage hierarchy and assigns it to the vector $\vec{i}$,
which serves as an input to subsequent operations. The $select(n,t)$ operation
iterates over each element $i \in \vec{i}$, and chooses $n$ distinct items of
type $t$ in the subtree rooted at that point. Storage devices have a known,
fixed type, and each bucket in the system has a type field that is used to
distinguish between classes of buckets (e. g., those representing ``rows'' and
those representing ``cabinets'').  For each $i \in \vec{i}$, the $select(n,t)$
call iterates over the $r \in 1,\dots,n$ items requested  and  recursively
descends through any intermediate buckets, pseudo-randomly selecting a nested
item in each bucket using the function $c(r,x)$ (defined for each kind of
bucket in \ref{bucket_types}), until it finds an item of the requested type
$t$. The resulting $n|\vec{i}|$ distinct items are placed back into the input
$\vec{i}$ and either form the input for a subsequent $select(n,t)$ or are moved
into the result vector with an $emit$ operation.

\begin{table}[h!]
\centering
\begin{tabular}{ |c|c| }
	\hline
	\emph{Action} & \emph{Resulting $\vec{i}$} \\
	\hline
	$take(root)$ & root \\
	$select(1,row)$ & row2 \\
	$select(3,cabinet)$ & cab21 cab23 cab24 \\
	$select(1,disk)$ & disk2107 disk2313 disk2437 \\
	$emit$ & \\
	\hline
\end{tabular}
\caption{A simple rule that distributes three replicas across three cabinets in the same row.}
\label{table:1}
\end{table}

As an example, the rule defined in Table \ref{table:1} begins at the root of the hierarchy
in Figure 1 and with the first $select(1,row)$ chooses a single bucket of type
``row'' (it selects row 2). The subsequent $select(3,cabinet)$ chooses three
distinct cabinets nested  beneath the previously selected $row2 (cab21, cab23,
cab24)$, while the final $select(1,disk)$ iterates over the three cabinet
buckets in the input vector and chooses a single disk nested beneath each of
them. The final result is three disks spread over three cabinets, but all in
the same row. This approach thus allows replicas to be simultaneously separated
across and constrained within container types (e.g. rows, cabinets, shelves), a
useful property for both reliability and performance considerations. Rules
consisting of multiple $take$, $emit$ blocks allow storage targets to be
explicitly drawn from different pools of storage, as might be expected in
remote replication scenarios (in which one replica is stored at a remote site)
or tiered installations (e. g., fast, near-line storage and slower,
higher-capacity arrays).

\subsubsection{Collisions, Failure, and Overload}
The $select(n,t)$ operation may traverse many levels of the storage hierarchy
in order to locate $n$ distinct items of the specified type $t$ nested beneath
its starting point, a recursive process partially parameterized by $r =
1,\dots,n$, the replica number being chosen. During this process, CRUSH may
reject and reselect items using a modified input $r'$ for three different
reasons: if an item has already been selected in the current set (a collision
--- the $select(n,t)$ result must be distinct), if a device is failed, or if a
device is overloaded. Failed or overloaded devices are marked as such in the
cluster map, but left in the hierarchy to avoid unnecessary shifting of data.
CRUSH’s selectively diverts a fraction of an overloaded device’s data by
pseudo-randomly rejecting with the probability specified in the cluster
map-typically related to its reported over-utilization. For failed or
overloaded devices, CRUSH uniformly redistributes items across the storage
cluster by restarting the recursion at the beginning of the $select(n, t)$ (see
Algorithm \ref{algorithm:1} line 11). In the case of collisions, an alternate
$r'$ is used first at inner levels of the recursion to attempt a local search
(see Algorithm \ref{algorithm:1} line 14) and avoid skewing the overall data
distribution away from subtrees where collisions are more probable (e.g., where
buckets are smaller than $n$).

\section{Self Check Questions}

\begin{itemize}
	\item What is RADOS?

	\item What daemons does ceph cluster consist of?

	\item How does ceph store objects on disk?

	\item Describe the process of storing an object in the cluster.

	\item List four maps that are most essential to a ceph cluster. Give
		the commands for getting those maps from the cluster.

	\item What parameters are required for creating a pool?

	\item Describe the placement group mapping process.

	\item Describe how location of an object is computed.
\end{itemize}

\end{document}
